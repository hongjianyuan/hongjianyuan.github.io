<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Fully Convolutional Networks for Semantic Segmentation</title>
    <url>/2020/07/03/FCN/</url>
    <content><![CDATA[<ul>
<li><strong>Authors:</strong>     Jonathan Long，Evan Shelhamer，Trevor Darrell</li>
<li><strong>Publish:</strong> CVPR2015</li>
<li><a href="https://arxiv.org/pdf/1411.4038.pdf" target="_blank" rel="noopener">Link</a></li>
</ul>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>1.建立一个全卷积的神经网路。</p>
<p>2.输入任意尺寸，经过有效的推理和学习产生相应尺寸的输出。</p>
<p>3.定义了一个跳跃式的架构，结合来自深度网络层语义信息和来自原始网络表征信息来产生准确的分割。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>首次运用全卷积进行预测</p>
<p>在网络中上采样层进行像素级别预测，而下采样池化来进行学习。</p>
<p>将FCN通过经典的CNN分类网络（VGG）进行改造并且微调。</p>
<p>语义分割面临着语义和定位的两难问题。全局信息解决的是什么，定位信息解决的是在哪里。</p>
<p>定义了一个跳跃式的架构，结合来自深度网络层语义信息和来自原始网络表征信息来产生准确的分割。</p>
<p><img src="/2020/07/03/FCN/FCN1.png" alt></p>
<h3 id="Fully-convolutional-networks："><a href="#Fully-convolutional-networks：" class="headerlink" title="Fully convolutional networks："></a><strong>Fully convolutional networks：</strong></h3><p>卷积网的每层数据是一个h * w * d的三维数组，其中h和w是空间维度,d是特征或通道维数。第一层是像素尺寸为h * w、颜色通道数为d的图像。高层中的locations和图像中它们连通的locations相对应，被称为接收域。</p>
<p><img src="/2020/07/03/FCN/FCN2.png" alt></p>
<p>这些完全连接的层也可以被视为与覆盖整个输入区域的内核的卷积。 这样做将它们转换为完全卷积网络，可以输入任意大小和输出分类图</p>
<p><strong>上采样是反卷积</strong></p>
<p>首先我们来看一下卷积</p>
<p>其中蓝色的图片(4 * 4)表示的是进行卷积的图片，阴影的图片(3 * 3)表示的是卷积核，绿色的图片(2*2)表示是进行卷积计算之后的图片。</p>
<p><img src="/2020/07/03/FCN/%E5%8D%B7%E7%A7%AF.gif" alt></p>
<p>反卷积</p>
<p><img src="/2020/07/03/FCN/%E5%8F%8D%E5%8D%B7%E7%A7%AF.gif" alt></p>
<p>乍看一下好像反卷积和卷积的工作过程差不多，主要的区别在于反卷积输出图片的尺寸会大于输入图片的尺寸，通过增加padding来实现这一操作，上图展示的是一个strides(步长)为1的反卷积。FCN首先对特征图各神经元之间进行0填充，即上池化；然后再进行卷积运算。计算公式为：(W1−1)×S-2×P+F=W2，根据前一池化层上采样的结合实现像素的密集预测</p>
<p><img src="/2020/07/03/FCN/FCN3.jpg" alt></p>
<ol>
<li>对于FCN-32s，直接对pool5 feature进行32倍上采样获得32x upsampled     feature，再对32x upsampled feature每个点做softmax prediction获得32x upsampled     feature prediction（即分割图）。</li>
<li>对于FCN-16s，首先对pool5     feature进行2倍上采样获得2x upsampled feature，再把pool4 feature和2x upsampled feature逐点相加，然后对相加的feature进行16倍上采样，并softmax     prediction，获得16x upsampled feature prediction。</li>
<li>对于FCN-8s，首先进行pool4+2x     upsampled feature逐点相加，然后又进行pool3+2x     upsampled逐点相加，即进行更多次特征融合。</li>
</ol>
<p>作者在原文种给出3种网络结果对比，效果：FCN-32s &lt; FCN-16s &lt; FCN-8s，即使用多层feature融合有利于提高分割准确性。</p>
<p>如果不在conv1_1加入pad=100，那么对于小于192x192的输入图像，在反卷积恢复尺寸前已经feature map size = 0！所以在conv1_1添加pad=100的方法，解决输入图像大小的问题（但是实际也引入很大的噪声）。</p>
<p>由于FCN在conv1_1加入pad=100，同时fc6卷积层也会改变feature map尺寸，那么真实的网络就不可能像原理图3那样“完美1/2”。</p>
<p>那么在特征融合的时候，如何保证逐点相加的feature map是一样大的呢？这就要引入crop层了。</p>
]]></content>
      <categories>
        <category>图像分割</category>
        <category>人工智能</category>
        <category>计算机视觉</category>
        <category>深度学习</category>
        <category>论文</category>
      </categories>
  </entry>
  <entry>
    <title>Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation</title>
    <url>/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/</url>
    <content><![CDATA[<ul>
<li><strong>Authors:</strong>     George Papandreou, Liang-Chieh Chen, Kevin Murphy，Alan L. Yuille</li>
<li><strong>Publish:</strong>       ICCV 2015</li>
<li><strong><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Papandreou_Weakly-_and_Semi-Supervised_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Link</a></strong></li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><strong>1.作者想解决什么问题？</strong></p>
<p>答：（1）弱注释来训练数据（例如从边界框或图像级别标签来训练出分割的像素级类别）。</p>
<p>​        （2）从一个或者多个数据集的少量强标注图像和许多弱标注图像进行组合。</p>
<p>从这两方来解决学习DCNN用于语义分割的问题。</p>
<p><strong>2.作者通过什么理论/模型来解决这个问题？</strong></p>
<p>答：在弱监督或者半监督的环境下，使用期望最大化（EM）训练DeepLab+CRF。</p>
<p><strong>3.作者给出的答案是什么？</strong></p>
<p>答：在PASCAL VOC 2012图像分割的基准上，得到了有竞争力的结果，并且明显的减少了标注的工作量。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​        语义图像分割是指给图像中的<strong>每个像素</strong>分配一个语义标签（如“人”、“车”或“狗”）。根据具有挑战性的Pascal VOC 2012分割基准测试的结果，性能最好的方法都使用某种<strong>深度卷积神经网络</strong>（DCNN）。本文使用的是<strong>DeepLab CRF方法</strong>，这个在VOC2012中mIOU为70%的模型。</p>
<p>​        DCNN的方法瓶颈是通常需要强标注信息，但是强标注信息工作量大，为此，我们使用更容易获取的弱监督标签来训练DCNN模型。</p>
<p>​        使用的数据形式的Image-level标签，这种形式主要是指定是否存在某个类别，而不是像素的位置，弱监督学习大多采用的是多实例学习（MIL）的方法。但是目前来说，弱监督学习的效果仍然大大落后于强监督。</p>
<p>​        本文开发一种期望最大化（EM）方法用于从弱注释数据中训练DCNN模型。所提出的算法在估计潜在像素标签（受弱注释约束）和使用随机梯度下降（SGD）优化DCNN参数之间交替。该算法在获取少量强（像素级）注释图像和大量弱（边界框或图像级）注释图像的情况下，几乎可以与全监督系统的性能相匹配。</p>
<p>​        本文主要贡献：1.提出了一种基于image-level或边界框标注的EM算法，适用于弱监督和半监督环境。2.结果表明，该方法在将少量像素级标注图像与大量图像级或边界框标注图像相结合时取得了很好的效果，几乎与所有训练图像都有像素级标注的结果相匹配。3.我们展示了在数据集中结合弱注释或强注释可以得到进一步的改进.特别是，通过结合来自PASCAL和MS-COCO数据集的注释，我们在PASCAL VOC 2012上的IOU性能达到73.9%。</p>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p>m表示像素，Y<sub>m</sub>表示m像素的标签，如果图片上出现了第L个标签出现在图像任何一个地方，则Z<sub>L</sub>=1</p>
<h4 id="1-Pixel-level-annotations（强监督）"><a href="#1-Pixel-level-annotations（强监督）" class="headerlink" title="1. Pixel-level annotations（强监督）"></a>1. Pixel-level annotations（强监督）</h4><p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/1.JPG" alt="1"></p>
<p>此过程目标函数为：</p>
<p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/2.JPG" alt="2"></p>
<p>θ是DCNN的参数，每一个像素的标签分布如下：</p>
<p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/4.JPG" alt></p>
<p>f<sub>m</sub>(y<sub>m</sub>|x;θ)是DCNN在m处的输出，通过mini-batch SGD来优化函数。</p>
<h4 id="2-Image-level-annotations"><a href="#2-Image-level-annotations" class="headerlink" title="2. Image-level annotations"></a>2. Image-level annotations</h4><p>当只有图像级注释可用时，我们可以观察到图像值x和图像级别标签z，但像素级分割结果y是潜在变量。使用下面概率图：</p>
<p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/5.png" alt="5"></p>
<p>我们采用EM的方法从训练数据学习模型参数θ，如果我们忽略不依赖θ项，那么给定先前参数估计值θ<sup>‘</sup>，则完全数据的期望对数似然函数为：</p>
<p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/6.JPG" alt="6"></p>
<p>之后采用hard-EM近似评估E-step：</p>
<p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/7.JPG" alt></p>
<p>在算法的M-step中，我们采用类似于mini-batch的SGD来优化Q函数<em>Q</em>(<strong>θ</strong>;<strong>θ</strong>′)≈log<em>P</em>(<strong>y</strong>^∣<strong>x</strong>;<strong>θ</strong>)，并将y<sup>‘</sup>作为分割的真实值。</p>
<p>（1）EM-Fixed</p>
<p>在这个变体中，我们假设logP（z | y）将像素位置分解为</p>
<p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/8.JPG" alt></p>
<p>因此每个像素的估计值y<sup>^</sup>m可以表示为：</p>
<p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/9.JPG" alt></p>
<p>假设：</p>
<p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/10.JPG" alt></p>
<p>我们设置参数b<sub>l</sub>=b<sub>fg</sub>,如果l&gt;0,b<sub>0</sub>=b<sub>bg</sub>,则b<sub>fg</sub>&gt;b<sub>bg</sub>&gt;0.直观地说，这种可能性鼓励一个像素被分配给一个图像级别的标签z。我们选择b<sub>fg</sub>&gt;b<sub>bg</sub>，使当前的前景类比背景更强大，以鼓励完全覆盖对象，并避免将所有像素分配给背景的退化解决方案。该过程在算法1中进行了总结，并在图2中进行了说明。</p>
<p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/3.jpg" alt></p>
<p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/11.JPG" alt></p>
<p>​    (2)EM-Adapt</p>
<p>在这种方法中，我们假设logP(z|y) =φ(y,z) + (const),其中φ（y，z）采用基数势的形式。特别地，我们鼓励将图像区域的至少一个ρl部分分配给类l（如果zl=1），并强制在zl=0时不将像素分配给类l。我们设置参数ρ<sub>l</sub>=ρ<sub>fg</sub>，如果l&gt;0且ρ<sub>0</sub>=ρ<sub>bg</sub>。类似的约束出现在中。</p>
<p>在实际应用中，我们采用了算法1的一个变体。我们自适应地设置与图像和类相关的偏差bl，以便将图像区域的指定比例分配给背景或前景对象类。这是一个强大的约束，显着地阻止了背景分数在整个图像中的流行，也促进了更高的前景目标覆盖率。详细的算法在补充材料中进行了描述。</p>
<h4 id="3-Bounding-Box-Annotations"><a href="#3-Bounding-Box-Annotations" class="headerlink" title="3. Bounding Box Annotations"></a>3. Bounding Box Annotations</h4><p>我们探索了三种方法来从边界框标注训练我们的分割模型</p>
<p>（1）Bbox-Rect</p>
<p>相当于简单的将边界框中的每个像素作为各自对象的positive，通过将属于多个边界框的像素指定给面积最小的一个边界框，解决歧义。边界框完全包围对象，但也包含背景像素，这些像素会用通过各个对象类的假阳性示例来污染训练集。</p>
<p>（2）Bbox-Seg</p>
<p>为了过滤掉这些背景像素，我们执行自动前景/背景分割。为了进行这种分割，我们使用与DeepLab相同的CRF。更具体地说，我们将边界框的中心区域（框内像素的α%）约束为前景，而将边界框外的像素约束为背景。我们通过适当设置CRF的一元项来实现这一点。然后推断中间像素的标签。我们交叉验证CRF参数，以最大限度地提高分割精度在一个小的持有全注释图像集。</p>
<p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/12.JPG" alt></p>
<p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/13.JPG" alt></p>
<p>(3)Bbox-EM-Fixed</p>
<p>它允许我们在整个训练过程中细化估计的分割图。该方法是Sec 2中EM Fixed算法的一个变种。我们只在边界框区域内提高当前前景对象的分数。</p>
<h4 id="4-Mixed-strong-and-weak-annotations"><a href="#4-Mixed-strong-and-weak-annotations" class="headerlink" title="4. Mixed strong and weak annotations"></a>4. Mixed strong and weak annotations</h4><p>在实践中，我们通常可以访问大量的弱图像级注释图像，并且只能为这些图像的一小部分获取详细的像素级注释。我们通过结合前面几节中介绍的方法，如图5所示。在我们的深层CNN模型的SGD训练中，我们将一个固定比例的强/弱注释图像捆绑到每个小批量中，并在每次迭代时使用EM算法估计弱注释图像的潜在语义分割。</p>
<p><img src="/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/14.JPG" alt></p>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>本文探讨了在训练一个最新的语义图像分割模型时使用弱或部分注释。在具有挑战性的PASCAL VOC  2012数据集上进行的大量实验表明：（1）仅在图像级别使用弱注释似乎不足以训练出高质量的分割模型。（2）  在训练集中使用弱边界框注释和仔细的图像分割推理，足以训练一个竞争模型。（3）  在半监督环境下，将少量的像素级注释图像与大量弱注释图像相结合，可以获得很好的性能，几乎与所有训练图像都有像素级注释时的结果相匹配。（4）  从其他数据集中利用额外的弱注释或强注释可以带来很大的改进。</p>
]]></content>
      <categories>
        <category>图像分割</category>
        <category>人工智能</category>
        <category>计算机视觉</category>
        <category>深度学习</category>
        <category>论文</category>
        <category>弱监督</category>
      </categories>
  </entry>
  <entry>
    <title>卷积神经网络（CNN）讲解</title>
    <url>/2020/07/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89%E8%AE%B2%E8%A7%A3/</url>
    <content><![CDATA[<p>本文主要是对于花书的第9章《卷积网络》所做的笔记，并且结合网络上相关资料，侵删。</p>
<p><strong>卷积神经网络</strong>（Convolutional neural network，CNN）（LeCun，1989），是一种专门处理用来具有类似网格结构的数据的神经网络。例如时间序列数据（一维）或者图像数据（二维），在诸多应用领域都表现优异。卷积是一种特殊的线性运算，是指那些至少在网络的一层中使用卷积运算来代替一般的矩阵乘法运算的神经网络。</p>
<h3 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h3><p>例子引入：假设我们正在用激光传感器追踪一艘宇宙飞船的位置。我们的激光传感器给出一个单独的输出x(t)，表示宇宙飞船在时刻t 的位置。x 和t 都是实值的，这意味着我们可以在任意时刻从传感器中读出飞船的位置。现在假设我们的传感器含有噪声。为了得到飞船位置的低噪声估计，我们对得到的测量结果进行平均。显然，时间上越近的测量结果越相关，所以我们采用一种加权平均的方法，对于最近的测量结果赋予更高的权值。我们可以采用一个加权函数w(a) 来实现，其中a 表示测量结果据当前时刻的时间间隔。如果我们对任意时刻都采用这种加权平均的操作，就得到了对于飞船位置的连续估计函数s：</p>
<p><img src="/2020/07/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89%E8%AE%B2%E8%A7%A3/1.JPG" alt></p>
<p>这种运算就叫做卷积(convolution)。卷积运算通常用星号表示：</p>
<p><img src="/2020/07/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89%E8%AE%B2%E8%A7%A3/2.JPG" alt></p>
<p>在卷积神经网络的术语中，第一个参数（在这个例子中，函数x）叫做<strong>输入</strong>(input)，第二个参数（函数w）叫做<strong>核函数</strong>(kernel function)。输出有时被称作<strong>特征映射</strong>（feature map）。</p>
<p>核函数的定义：<strong>对于输入图像中的一部分区域，进行加权求和的处理，其中这个过程的权重，由一个函数定义，这个函数就是卷积核</strong></p>
<p>在我们的例子中，连续时间是不现实的，通常采用离散时间（就是每隔几秒反馈一次），所以时间t 只能<br>取整数值。如果我们假设x 和w 都定义在整数时刻t 上，就得到了离散形式的卷积：</p>
<p><img src="/2020/07/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89%E8%AE%B2%E8%A7%A3/3.JPG" alt></p>
<p>在机器学习的应用中，输入通常是高维数据数组，而核也是由算法产生的高维参数数组。我们把这种高维数组叫做张量。因为输入与核的每一个元素都分开存储，我们经常假设在存储了数据的有限点集以外，这些函数的值都为零。这意味着在实际操作中，我们可以统一地把无限的求和当作对有限个数组元素的求和来用。</p>
<p>如果把二维的图像I 作为输入，我们也相应的需要使用二维的核K：</p>
<p><img src="/2020/07/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89%E8%AE%B2%E8%A7%A3/4.JPG" alt></p>
<p><img src="/2020/07/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89%E8%AE%B2%E8%A7%A3/5.JPG" alt></p>
<p>上图展示了一个在二维张量上的卷积运算的例子。是不是这张图比较难以理解，我们放在动图帮助理解。</p>
<p><img src="/2020/07/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89%E8%AE%B2%E8%A7%A3/11.gif" alt></p>
<h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>卷积运算通过三个重要的思想来帮助改进机器学习系统：稀疏交互(sparse interactions)、参数共享(parameter sharing)、等变表示(equivariant representations)。</p>
<h4 id="稀疏交互"><a href="#稀疏交互" class="headerlink" title="稀疏交互"></a>稀疏交互</h4><p>卷积神经网络具有稀疏交互(sparse interactions)（也叫做稀疏连接(sparse connectivity) 或者稀疏权重(sparse weights)）的特征。这通过使得核的规模远小于输入的规模来实现。</p>
<h4 id="参数共享"><a href="#参数共享" class="headerlink" title="参数共享"></a>参数共享</h4><p>在卷积神经网络中，核的每一个元素都作用在输入的每一个位置。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一个位置都需要学习一个单独的参数集合。</p>
<h4 id="等变"><a href="#等变" class="headerlink" title="等变"></a>等变</h4><p>对于卷积，参数共享的特殊形式是的神经网络具有对平移等变得性质。如果一个函数满足输入改变，输出也以同样的方式改变这一性质，我们就说它是等变的。</p>
<h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p>池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。</p>
<p><strong>最大池化(max pooling)</strong> 函数(Zhou and Chellappa, 1988) 给出相邻矩形区域内的最大值。</p>
<p><img src="/2020/07/04/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89%E8%AE%B2%E8%A7%A3/8.jpg" alt></p>
<p>其他常用的池化函数包括相邻矩形区域内的平均值、L2 范数以及依靠据中心像素距离的加权平均函数。不管采用什么样的池化函数，当输入作出少量平移时，池化能帮助我们的表示近似不变(invariant)。</p>
<h3 id="卷积与池化作为一种无限强的先验"><a href="#卷积与池化作为一种无限强的先验" class="headerlink" title="卷积与池化作为一种无限强的先验"></a>卷积与池化作为一种无限强的先验</h3><p>先验被认为是强或者弱取决于先验中概率密度的集中程度。弱先验具有较高的熵值，例如方差很大的高斯分布，这样的先验允许数据对于参数的改变具有或多或少的自由性。强先验具有较低的熵值，例如方差很小的高斯分布，样的先验在决定参数最终取值时起着更加积极的作用。</p>
<p>但把卷积神经网络想成具有无限强先验的全连接网络可以帮助我们更好地洞察卷积神经网络是如何工作的。其中一个关键的洞察是卷积和池化可能导致欠拟合。另一个关键洞察是当我们比较卷积模型的统计学习表现时，只能以基准中的其他卷积模型作为比较的对象。</p>
<h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><table>
<thead>
<tr>
<th></th>
<th>单通道</th>
<th>多通道</th>
</tr>
</thead>
<tbody><tr>
<td>1 维</td>
<td>音频波形：卷积的轴对应于时间。我们将时间离散化并且在每个时间点测量一次波形的振幅</td>
<td>骨架动画(skeleton animation) 数据：计算机渲染的3D 角色动画是通过随时间调整‘‘骨架’’ 的姿势而生成的。在每个时间点，角色的姿势通过骨架中的每个关节的角度来描述。我们输入到卷积模型的数据的每个通道，表示一个关节的关于一个轴的角度。</td>
</tr>
<tr>
<td>2 维</td>
<td>已经用傅立叶变换预处理的音频数据：我们可以将音频波形变换成2 维张量，不同的行对应不同的频率，不同的列对应不同的时间点。在时间轴上使用卷积使模型等效于在时间上移动。在频率轴上使用卷积使得模型等效于在频率上移动，这使得在不同八度音阶中播放的相同旋律产生相同的表示，但处于网络输出中的不同高度。</td>
<td>彩色图像数据：其中一个通道包含红色像素，另一个包含绿色像素，最后一个包含蓝色像素。在图像的水平轴和竖直轴上移动卷积核，赋予了两个方向上平移等变性。</td>
</tr>
<tr>
<td>3 维</td>
<td>体积数据：这种数据一般来源于医学成像技术，例如CT 扫描等。</td>
<td>彩色视频数据：其中一个轴对应着时间，另一个轴对应着视频帧的高度，最后一个对应着视频帧的宽度。</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>计算机视觉</category>
        <category>深度学习</category>
      </categories>
  </entry>
</search>
