<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>您好！</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-07-03T05:47:27.113Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>HongJianYuan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation</title>
    <link href="http://yoursite.com/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/"/>
    <id>http://yoursite.com/2020/07/03/Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation/</id>
    <published>2020-07-03T02:54:26.000Z</published>
    <updated>2020-07-03T05:47:27.113Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>Authors:</strong>     George Papandreou, Liang-Chieh Chen, Kevin Murphy，Alan L. Yuille</li><li><strong>Publish:</strong>       ICCV 2015</li><li><strong><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Papandreou_Weakly-_and_Semi-Supervised_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Link</a></strong></li></ul><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><strong>1.作者想解决什么问题？</strong></p><p>答：（1）弱注释来训练数据（例如从边界框或图像级别标签来训练出分割的像素级类别）。</p><p>​        （2）从一个或者多个数据集的少量强标注图像和许多弱标注图像进行组合。</p><p>从这两方来解决学习DCNN用于语义分割的问题。</p><p><strong>2.作者通过什么理论/模型来解决这个问题？</strong></p><p>答：在弱监督或者半监督的环境下，使用期望最大化（EM）训练DeepLab+CRF。</p><p><strong>3.作者给出的答案是什么？</strong></p><p>答：在PASCAL VOC 2012图像分割的基准上，得到了有竞争力的结果，并且明显的减少了标注的工作量。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​        语义图像分割是指给图像中的<strong>每个像素</strong>分配一个语义标签（如“人”、“车”或“狗”）。根据具有挑战性的Pascal VOC 2012分割基准测试的结果，性能最好的方法都使用某种<strong>深度卷积神经网络</strong>（DCNN）。本文使用的是<strong>DeepLab CRF方法</strong>，这个在VOC2012中mIOU为70%的模型。</p><p>​        DCNN的方法瓶颈是通常需要强标注信息，但是强标注信息工作量大，为此，我们使用更容易获取的弱监督标签来训练DCNN模型。</p><p>​        使用的数据形式的Image-level标签，这种形式主要是指定是否存在某个类别，而不是像素的位置，弱监督学习大多采用的是多实例学习（MIL）的方法。但是目前来说，弱监督学习的效果仍然大大落后于强监督。</p><p>​        本文开发一种期望最大化（EM）方法用于从弱注释数据中训练DCNN模型。所提出的算法在估计潜在像素标签（受弱注释约束）和使用随机梯度下降（SGD）优化DCNN参数之间交替。该算法在获取少量强（像素级）注释图像和大量弱（边界框或图像级）注释图像的情况下，几乎可以与全监督系统的性能相匹配。</p><p>​        本文主要贡献：1.提出了一种基于image-level或边界框标注的EM算法，适用于弱监督和半监督环境。2.结果表明，该方法在将少量像素级标注图像与大量图像级或边界框标注图像相结合时取得了很好的效果，几乎与所有训练图像都有像素级标注的结果相匹配。3.我们展示了在数据集中结合弱注释或强注释可以得到进一步的改进.特别是，通过结合来自PASCAL和MS-COCO数据集的注释，我们在PASCAL VOC 2012上的IOU性能达到73.9%。</p><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p>m表示像素，Y<sub>m</sub>表示m像素的标签，如果图片上出现了第L个标签出现在图像任何一个地方，则Z<sub>L</sub>=1</p><h4 id="1-Pixel-level-annotations（强监督）"><a href="#1-Pixel-level-annotations（强监督）" class="headerlink" title="1. Pixel-level annotations（强监督）"></a>1. Pixel-level annotations（强监督）</h4><p><img src="/.com//1.JPG" alt="1"></p><p>此过程目标函数为：</p><p><img src="/.com//2.JPG" alt="2"></p><p>θ是DCNN的参数，每一个像素的标签分布如下：</p><p><img src="/.com//4.JPG" alt></p><p>f<sub>m</sub>(y<sub>m</sub>|x;θ)是DCNN在m处的输出，通过mini-batch SGD来优化函数。</p><h4 id="2-Image-level-annotations"><a href="#2-Image-level-annotations" class="headerlink" title="2. Image-level annotations"></a>2. Image-level annotations</h4><p>当只有图像级注释可用时，我们可以观察到图像值x和图像级别标签z，但像素级分割结果y是潜在变量。使用下面概率图：</p><p><img src="/.com//Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation%5C5.png" alt="image-20200703114930043"></p><p>我们采用EM的方法从训练数据学习模型参数θ，如果我们忽略不依赖θ项，那么给定先前参数估计值θ<sup>‘</sup>，则完全数据的期望对数似然函数为：</p><p><img src="/.com//Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation%5C6.JPG" alt="6"></p><p>之后采用hard-EM近似评估E-step：</p><p><img src="/.com//Weakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation%5C7.JPG" alt></p><p>在算法的M-step中，我们采用类似于mini-batch的SGD来优化Q函数<em>Q</em>(<strong>θ</strong>;<strong>θ</strong>′)≈log<em>P</em>(<strong>y</strong>^∣<strong>x</strong>;<strong>θ</strong>)，并将y<sup>‘</sup>作为分割的真实值。</p><p>（1）EM-Fixed</p><p>在这个变体中，我们假设logP（z | y）将像素位置分解为</p><p><img src="/.com//D:%5Cblog%5Csource_posts%5CWeakly-and-Semi-Supervised-Learning-of-a-DCNN-for-Semantic-Image-Segmentation%5C8.JPG" alt></p><p>因此每个像素的估计值y<sup>^</sup>m可以表示为：</p><p><img src="/.com//9.JPG" alt></p><p>假设：</p><p><img src="/.com//10.JPG" alt></p><p>我们设置参数b<sub>l</sub>=b<sub>fg</sub>,如果l&gt;0,b<sub>0</sub>=b<sub>bg</sub>,则b<sub>fg</sub>&gt;b<sub>bg</sub>&gt;0.直观地说，这种可能性鼓励一个像素被分配给一个图像级别的标签z。我们选择b<sub>fg</sub>&gt;b<sub>bg</sub>，使当前的前景类比背景更强大，以鼓励完全覆盖对象，并避免将所有像素分配给背景的退化解决方案。该过程在算法1中进行了总结，并在图2中进行了说明。</p><p><img src="/.com//3.jpg" alt></p><p><img src="/.com//11.JPG" alt></p><p>​    (2)EM-Adapt</p><p>在这种方法中，我们假设logP(z|y) =φ(y,z) + (const),其中φ（y，z）采用基数势的形式。特别地，我们鼓励将图像区域的至少一个ρl部分分配给类l（如果zl=1），并强制在zl=0时不将像素分配给类l。我们设置参数ρ<sub>l</sub>=ρ<sub>fg</sub>，如果l&gt;0且ρ<sub>0</sub>=ρ<sub>bg</sub>。类似的约束出现在中。</p><p>在实际应用中，我们采用了算法1的一个变体。我们自适应地设置与图像和类相关的偏差bl，以便将图像区域的指定比例分配给背景或前景对象类。这是一个强大的约束，显着地阻止了背景分数在整个图像中的流行，也促进了更高的前景目标覆盖率。详细的算法在补充材料中进行了描述。</p><h4 id="3-Bounding-Box-Annotations"><a href="#3-Bounding-Box-Annotations" class="headerlink" title="3. Bounding Box Annotations"></a>3. Bounding Box Annotations</h4><p>我们探索了三种方法来从边界框标注训练我们的分割模型</p><p>（1）Bbox-Rect</p><p>相当于简单的将边界框中的每个像素作为各自对象的positive，通过将属于多个边界框的像素指定给面积最小的一个边界框，解决歧义。边界框完全包围对象，但也包含背景像素，这些像素会用通过各个对象类的假阳性示例来污染训练集。</p><p>（2）Bbox-Seg</p><p>为了过滤掉这些背景像素，我们执行自动前景/背景分割。为了进行这种分割，我们使用与DeepLab相同的CRF。更具体地说，我们将边界框的中心区域（框内像素的α%）约束为前景，而将边界框外的像素约束为背景。我们通过适当设置CRF的一元项来实现这一点。然后推断中间像素的标签。我们交叉验证CRF参数，以最大限度地提高分割精度在一个小的持有全注释图像集。</p><p><img src="/.com//12.JPG" alt></p><p><img src="/.com//13.JPG" alt></p><p>(3)Bbox-EM-Fixed</p><p>它允许我们在整个训练过程中细化估计的分割图。该方法是Sec 2中EM Fixed算法的一个变种。我们只在边界框区域内提高当前前景对象的分数。</p><h4 id="4-Mixed-strong-and-weak-annotations"><a href="#4-Mixed-strong-and-weak-annotations" class="headerlink" title="4. Mixed strong and weak annotations"></a>4. Mixed strong and weak annotations</h4><p>在实践中，我们通常可以访问大量的弱图像级注释图像，并且只能为这些图像的一小部分获取详细的像素级注释。我们通过结合前面几节中介绍的方法，如图5所示。在我们的深层CNN模型的SGD训练中，我们将一个固定比例的强/弱注释图像捆绑到每个小批量中，并在每次迭代时使用EM算法估计弱注释图像的潜在语义分割。</p><p><img src="/.com//14.JPG" alt></p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>本文探讨了在训练一个最新的语义图像分割模型时使用弱或部分注释。在具有挑战性的PASCAL VOC  2012数据集上进行的大量实验表明：（1）仅在图像级别使用弱注释似乎不足以训练出高质量的分割模型。（2）  在训练集中使用弱边界框注释和仔细的图像分割推理，足以训练一个竞争模型。（3）  在半监督环境下，将少量的像素级注释图像与大量弱注释图像相结合，可以获得很好的性能，几乎与所有训练图像都有像素级注释时的结果相匹配。（4）  从其他数据集中利用额外的弱注释或强注释可以带来很大的改进。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Authors:&lt;/strong&gt;     George Papandreou, Liang-Chieh Chen, Kevin Murphy，Alan L. Yuille&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Publish:&lt;/strong&gt;  
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
